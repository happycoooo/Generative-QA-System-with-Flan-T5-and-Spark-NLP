{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ec554e-3c0d-4a2c-a52d-b7d781bb6e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file_path = '/shareddata/data/project2/Task1/squad_v2/squad_v2/train-00000-of-00001.parquet'\n",
    "validation_file_path = '/shareddata/data/project2/Task1/squad_v2/squad_v2/validation-00000-of-00001.parquet'\n",
    "\n",
    "# Reading the parquet files into pandas DataFrames\n",
    "train_df = pd.read_parquet(train_file_path)\n",
    "test_df = pd.read_parquet(validation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5686d14d-5688-4882-8852-045040483805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe14b61907543e38b7f07f2b61244ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb74ba375184a879298c252edc4950e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aa33fc7cfe4f8b8e5f16e68ac0fe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_texts': 'question: What happened after Kanye made his controversial statement? context: Myers spoke next and continued to read the script. Once it was West\\'s turn to speak again, he said, \"George Bush doesn\\'t care about black people.\" At this point, telethon producer Rick Kaplan cut off the microphone and then cut away to Chris Tucker, who was unaware of the cut for a few seconds. Still, West\\'s comment reached much of the United States.', 'target_texts': 'Rick Kaplan cut off the microphone and then cut away to Chris Tucker'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "def process_squad_v2(data):\n",
    "    questions = data['question']\n",
    "    contexts = data['context']\n",
    "    answers = data['answers']\n",
    "    \n",
    "    inputs = [f\"question: {q} context: {c}\" for q, c in zip(questions, contexts)]\n",
    "    targets = [a[\"text\"][0] if len(a[\"text\"]) > 0 else \"no answer\" for a in answers]\n",
    "    return {\"input_texts\": inputs, \"target_texts\": targets}\n",
    "\n",
    "# 划分训练集和验证集 \n",
    "validation_size = 5000\n",
    "validation_df = train_df[:validation_size]\n",
    "train_df = train_df[validation_size:]\n",
    "\n",
    "train_inputs, train_targets = process_squad_v2(train_df)\n",
    "val_inputs, val_targets = process_squad_v2(validation_df)\n",
    "test_inputs, test_targets = process_squad_v2(test_df)\n",
    "\n",
    "# 转换为 HuggingFace Dataset 格式\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# 处理数据\n",
    "train_dataset = train_dataset.map(process_squad_v2, batched=True, remove_columns=train_dataset.column_names)\n",
    "validation_dataset = validation_dataset.map(process_squad_v2, batched=True, remove_columns=validation_dataset.column_names)\n",
    "test_dataset = test_dataset.map(process_squad_v2, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "# 检查处理后的数据\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1910ef-2f27-45f5-abe6-edca30e6541c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import os\n",
    "from ray.air import session\n",
    "from ray.air.config import ScalingConfig, RunConfig\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import ray.data as ray_data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from ray import tune\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset, DatasetDict\n",
    "model_path = \"/data/lab/project2/flan-t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741873d1-3197-4721-b4b1-048f83941a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 09:02:32,944\tWARNING services.py:2009 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67039232 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.48gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-06-03 09:02:34,151\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8267 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":task_name:get_table_block_metadata\n",
      ":actor_name:_StatsActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":task_name:get_table_block_metadata\n",
      ":actor_name:_StatsActor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":task_name:get_table_block_metadata\n",
      ":task_name:get_table_block_metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task_name:get_table_block_metadata\n",
      ":task_name:get_table_block_metadata\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path: str, *, include_label=True):\n",
    "    df = pd.read_csv(path)\n",
    "    arrow_table = pa.Table.from_pandas(df)\n",
    "    ray_dataset = ray.data.from_arrow(arrow_table)\n",
    "    return ray_dataset\n",
    "\n",
    "# Initial Ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, num_cpus=4,local_mode=True)\n",
    "\n",
    "# Process Dataset in Ray\n",
    "train_ray = load_dataset('./Q1_data/train_data.csv')\n",
    "test_ray = load_dataset('./Q1_data/test_data.csv')\n",
    "val_ray = load_dataset('./Q1_data/val_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d475fb1-db57-4adb-bcfd-85917c17c0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型和tokenizer加载成功\n"
     ]
    }
   ],
   "source": [
    "# 尝试加载本地模型和 tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
    "    print(\"模型和tokenizer加载成功\")\n",
    "except Exception as e:\n",
    "    print(f\"加载失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371efead-6949-45fe-9a9e-92689a599fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_length=512):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "\n",
    "        input_encodings = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        target_encodings = self.tokenizer(target_text, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "\n",
    "        input_ids = input_encodings['input_ids']\n",
    "        attention_mask = input_encodings['attention_mask']\n",
    "        labels = target_encodings['input_ids']\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = QADataset(train_inputs, train_targets, tokenizer)\n",
    "val_dataset = QADataset(val_inputs, val_targets, tokenizer)\n",
    "test_dataset = QADataset(test_inputs, test_targets, tokenizer)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca90def6-8eb9-4226-8036-d9e74d5ed0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  822,    10,   363,   844,   410,   493,    63, 14549,  5978,    16,\n",
       "           116,   255,    47,  1710,    95,    58,  2625,    10,   493,    63,\n",
       "           106,    75,   154,  3156,     7,   693,  8900,   965,    18,  6936,\n",
       "           449,    41,    87,   115,    23,     2,   354,     2,    29,     7,\n",
       "            15,     2,    87,    36,    15,    18,   476,  4170,    18,  8735,\n",
       "            61,    41,  7473,  1600,  6464, 15465,    61,    19,    46,   797,\n",
       "          7634,     6,     3, 21101,     6,  1368,  8211,    11, 15676,     5,\n",
       "         12896,    11,  3279,    16,  8018,     6,  2514,     6,   255,  3032,\n",
       "            16,   796,  8782,    11, 10410,  2259,     7,    38,     3,     9,\n",
       "           861,     6,    11,  4659,    12, 10393,    16,     8,  1480,  5541,\n",
       "             7,    38,   991,  7634,    13,   391,   184,   279,  3202,    18,\n",
       "         10739, 19344,    63,    31,     7,  9364,     5, 19607,    26,    57,\n",
       "           160,  2353,     6,  9762,    15,   210,  8900,   965,     6,     8,\n",
       "           563,  1632,    80,    13,     8,   296,    31,     7,   200,    18,\n",
       "         17556,  3202,  1637,    13,    66,    97,     5,  2940,  7102,   144,\n",
       "           302,  1509,     8,  1576,    13,   493,    63,   106,    75,   154,\n",
       "            31,     7,  5695,  2306,     6,  2744,  1304, 11937,    16,  2129,\n",
       "             3, 31210,     6,    84,  2127,   160,    38,     3,     9,  6729,\n",
       "          2377,  4388,     6,  4964,   874, 26596,  6580,    11,  4510,     8,\n",
       "          3259,  1976,  5396,   910,   381,    18,   782,   712,     7,    96,\n",
       "           254,  7275,    63,    16,  2129,   121,    11,    96,   279,     9,\n",
       "           969,  7508,  1280,     1,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([ 8782,    11, 10410,     1,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d202fb98-f1ea-4c5e-8a9b-ef57e29f6114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(batch, tokenizer, max_length=512):\n",
    "    inputs = list(batch[\"input_texts\"])\n",
    "    targets = list(batch[\"target_texts\"])\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\", text_target=targets)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_ray = train_ray.map_batches(tokenize_batch, batch_format=\"pandas\", fn_kwargs={\"tokenizer\": tokenizer})\n",
    "tokenized_val_ray = val_ray.map_batches(tokenize_batch, batch_format=\"pandas\", fn_kwargs={\"tokenizer\": tokenizer})\n",
    "tokenized_test_ray = test_ray.map_batches(tokenize_batch, batch_format=\"pandas\", fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a685612-2600-4dca-a032-76c52e6b6b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert Ray Datasets to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(tokenized_train_ray.to_pandas())\n",
    "val_dataset = Dataset.from_pandas(tokenized_val_ray.to_pandas())\n",
    "test_dataset = Dataset.from_pandas(tokenized_test_ray.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f3e552-a973-4891-8406-5857759b3ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save the tokenized datasets to disk\n",
    "dataset_dict.save_to_disk('./squad_v2_tokenized_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd84ed-172c-4676-83e1-3206fc6cf943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb151b5c-f5a3-4268-b54a-cdd9bdb48b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3109",
   "language": "python",
   "name": "py3109"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
